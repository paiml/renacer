# GPU Memory Transfer Tracking: Phase 4

**Version:** 1.0
**Date:** 2025-11-21
**Status:** Specification - Ready for Implementation
**Sprint Target:** 39 (GPU Memory Transfer Tracking)
**GitHub Issue:** #16 (Phase 4)
**Depends On:** Sprint 37 (Phase 1: wgpu kernel tracing)

## Executive Summary

This specification defines **GPU memory transfer observability** for **wgpu applications**, tracking CPUâ†”GPU data movement to identify PCIe bandwidth bottlenecks. Following **Phase 1's** kernel tracing and **Toyota Way** principles, this spec completes the GPU observability trifecta: **kernel execution + memory transfers + SIMD compute**.

**Business Value:**
- **Transfer Bottleneck Identification**: Identify slow CPUâ†”GPU transfers (often >10x slower than kernels)
- **PCIe Bandwidth Analysis**: Measure actual vs theoretical bandwidth utilization
- **Memory Optimization**: Guide decisions on buffer sizes, staging strategies
- **Complete GPU Timeline**: See when GPU is computing vs waiting for data

**Key Principle (Toyota Way):**
> *"Make the invisible visible."* - Memory transfers are often the hidden bottleneck. Trace them to find the truth.

---

## Table of Contents

1. [Background and Motivation](#1-background-and-motivation)
2. [Architecture Overview](#2-architecture-overview)
3. [Phase 4: Memory Transfer Tracking](#3-phase-4-memory-transfer-tracking)
4. [Implementation Plan](#4-implementation-plan)
5. [Testing Strategy](#5-testing-strategy)
6. [Performance Impact](#6-performance-impact)

---

## 1. Background and Motivation

### 1.1 The Hidden Bottleneck Problem

**Common Performance Anti-Pattern:**
```
GPU kernel: 5ms   âœ… Fast!
CPU â†’ GPU transfer: 45ms  âŒ 9x slower (hidden bottleneck)
GPU â†’ CPU transfer: 2ms   âœ… Acceptable
```

**Root Cause:** Developers focus on kernel optimization, miss transfer overhead.

### 1.2 Phase 1-3 Accomplishments

**âœ… Phase 1 Complete (Sprint 37):**
- wgpu GPU kernel tracing
- GpuKernel struct + record_gpu_kernel() method
- Adaptive sampling (100Î¼s threshold)
- 9 integration tests passing

**âœ… Phase 2 Specified (Sprint 38):**
- CUDA kernel tracing via CUPTI
- Blocked on NVIDIA GPU hardware

**âœ… Phase 3 Planned:**
- ROCm (AMD GPU) kernel tracing
- Similar to Phase 2, blocked on AMD hardware

**âŒ Memory Transfers Not Tracked:**
- CPU â†’ GPU buffer uploads invisible
- GPU â†’ CPU buffer downloads invisible
- PCIe bandwidth bottlenecks undetected

### 1.3 Use Case: Real-Time Graphics Pipeline

**Example application:** Game rendering with dynamic mesh updates

**Current visibility (Phase 1 only):**
```
Root Span: "process: game_engine"
â””â”€ Span: "gpu_kernel: vertex_shader" - 3ms  âœ… Traced
```

**Hidden bottleneck:**
```
CPU â†’ GPU: Upload mesh data - 25ms     âŒ NOT traced (bottleneck!)
GPU kernel: Process vertices - 3ms      âœ… Traced
GPU â†’ CPU: Readback framebuffer - 1ms  âŒ NOT traced
```

**Desired visibility (Phase 4):**
```
Root Span: "process: game_engine"
â”œâ”€ Span: "gpu_transfer: mesh_upload" (CPUâ†’GPU) - 25ms           ğŸ¯ NEW Phase 4
â”‚   â”œâ”€ gpu_transfer.direction: "cpu_to_gpu"
â”‚   â”œâ”€ gpu_transfer.bytes: 10485760  (10MB)
â”‚   â”œâ”€ gpu_transfer.bandwidth_mbps: 419.4  (25ms for 10MB)
â”‚   â””â”€ gpu_transfer.is_slow: true  (expected <5ms)
â”œâ”€ Span: "gpu_kernel: vertex_shader" - 3ms                      âœ… Phase 1
â””â”€ Span: "gpu_transfer: framebuffer_readback" (GPUâ†’CPU) - 1ms   ğŸ¯ NEW Phase 4
    â”œâ”€ gpu_transfer.direction: "gpu_to_cpu"
    â”œâ”€ gpu_transfer.bytes: 8294400  (7.9MB)
    â””â”€ gpu_transfer.bandwidth_mbps: 8294.4  (1ms for 8MB)
```

**Insight:** Mesh upload (25ms) is 8.3x slower than kernel execution (3ms) â†’ optimize transfer strategy!

### 1.4 Transfer Types in wgpu

**CPU â†’ GPU (Uploads):**
- `queue.write_buffer()` - Immediate copy to staging, then GPU
- `queue.write_texture()` - Texture uploads
- `encoder.copy_buffer_to_buffer()` - GPU-side copy (fast, already on GPU)

**GPU â†’ CPU (Downloads):**
- `buffer.map_async()` - Asynchronous readback
- `buffer.slice().get_mapped_range()` - Access mapped data

**GPU â†” GPU (Internal):**
- `encoder.copy_buffer_to_buffer()` - Already tracked by Phase 1 (part of command buffer)

**Phase 4 Scope:** Track CPU â†” GPU transfers only (the slow ones).

---

## 2. Architecture Overview

### 2.1 Integration Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Observability Backend (Jaeger, Tempo, etc.)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–²
                          â”‚ OTLP Protocol
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Renacer OTLP Exporter (src/otlp_exporter.rs)               â”‚
â”‚  - Export syscall spans                  âœ… Sprint 30       â”‚
â”‚  - Export SIMD compute blocks            âœ… Sprint 32       â”‚
â”‚  - Export wgpu GPU kernels               âœ… Sprint 37       â”‚
â”‚  - NEW: Export GPU memory transfers      ğŸ¯ Sprint 39       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–²
                          â”‚ record_gpu_transfer(GpuMemoryTransfer)
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPU Transfer Tracker (EXTEND: src/gpu_tracer.rs)           â”‚
â”‚  - Wrapper methods: traced_write_buffer(), etc.             â”‚
â”‚  - Wall-clock timing (std::time::Instant)                   â”‚
â”‚  - Convert transfer metadata â†’ GpuMemoryTransfer struct     â”‚
â”‚  - Adaptive sampling (same 100Î¼s threshold)                 â”‚
â”‚  - Export as OTLP spans                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–²
                          â”‚ User calls wrappers instead of direct wgpu
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User's wgpu Application                                    â”‚
â”‚  - Replace: queue.write_buffer()                            â”‚
â”‚  - With: transfer_tracker.traced_write_buffer()            â”‚
â”‚  - Minimal code changes                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Span Hierarchy (Complete GPU Observability)

**Timeline with kernels + transfers:**
```
Root Span: "process: game_engine"
â”œâ”€ Span: "gpu_transfer: mesh_data_upload" (CPUâ†’GPU) - 25ms
â”‚   â”œâ”€ gpu_transfer.direction: "cpu_to_gpu"
â”‚   â”œâ”€ gpu_transfer.bytes: 10485760
â”‚   â”œâ”€ gpu_transfer.bandwidth_mbps: 419.4
â”‚   â””â”€ gpu_transfer.buffer_usage: "VERTEX"
â”œâ”€ Span: "gpu_kernel: vertex_shader" - 3ms
â”‚   â”œâ”€ gpu.backend: "wgpu"
â”‚   â”œâ”€ gpu.kernel: "vertex_shader"
â”‚   â””â”€ gpu.duration_us: 3000
â”œâ”€ Span: "gpu_kernel: fragment_shader" - 2ms
â””â”€ Span: "gpu_transfer: framebuffer_readback" (GPUâ†’CPU) - 1ms
    â”œâ”€ gpu_transfer.direction: "gpu_to_cpu"
    â”œâ”€ gpu_transfer.bytes: 8294400
    â””â”€ gpu_transfer.bandwidth_mbps: 8294.4
```

**Benefits:**
- âœ… **Complete GPU timeline**: See kernels AND transfers
- âœ… **Bottleneck identification**: Transfer (25ms) >> kernel (3ms)
- âœ… **Bandwidth analysis**: Actual (419 MB/s) vs theoretical (PCIe 4.0: 32 GB/s)

### 2.3 Span Attributes

**Resource-Level Attributes (once at startup):**
```json
{
  "resource": {
    "service.name": "renacer",
    "gpu.library.wgpu": "23.0.0",
    "gpu.tracing.capabilities": "kernels,transfers"
  }
}
```

**Span-Level Attributes (per transfer):**
```json
{
  "span.name": "gpu_transfer: mesh_data_upload",
  "span.kind": "INTERNAL",
  "attributes": {
    "gpu_transfer.direction": "cpu_to_gpu",
    "gpu_transfer.bytes": 10485760,
    "gpu_transfer.duration_us": 25000,
    "gpu_transfer.bandwidth_mbps": 419.4,
    "gpu_transfer.buffer_usage": "VERTEX",
    "gpu_transfer.is_slow": true,
    "gpu_transfer.threshold_us": 100
  },
  "status": "OK"
}
```

---

## 3. Phase 4: Memory Transfer Tracking

### 3.1 New Data Structure: `GpuMemoryTransfer`

**File:** `src/otlp_exporter.rs` (extend existing)

```rust
/// GPU memory transfer direction
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum TransferDirection {
    /// CPU â†’ GPU (buffer upload)
    CpuToGpu,
    /// GPU â†’ CPU (buffer download/readback)
    GpuToCpu,
}

impl TransferDirection {
    pub fn as_str(&self) -> &'static str {
        match self {
            TransferDirection::CpuToGpu => "cpu_to_gpu",
            TransferDirection::GpuToCpu => "gpu_to_cpu",
        }
    }
}

/// GPU memory transfer metadata for tracing (Sprint 39 - Phase 4)
///
/// Represents a single CPUâ†”GPU memory transfer operation captured via wall-clock timing.
#[derive(Debug, Clone)]
pub struct GpuMemoryTransfer {
    /// Transfer name/label (e.g., "mesh_data_upload", "framebuffer_readback")
    pub label: String,
    /// Transfer direction (CPUâ†’GPU or GPUâ†’CPU)
    pub direction: TransferDirection,
    /// Number of bytes transferred
    pub bytes: usize,
    /// Total duration in microseconds
    pub duration_us: u64,
    /// Calculated bandwidth in MB/s
    pub bandwidth_mbps: f64,
    /// Optional buffer usage hint (e.g., "VERTEX", "UNIFORM", "STORAGE")
    pub buffer_usage: Option<String>,
    /// Whether this transfer exceeded the slow threshold (>100Î¼s)
    pub is_slow: bool,
}

impl GpuMemoryTransfer {
    /// Create a new GPU memory transfer record
    ///
    /// Automatically calculates bandwidth from bytes and duration.
    pub fn new(
        label: String,
        direction: TransferDirection,
        bytes: usize,
        duration_us: u64,
        buffer_usage: Option<String>,
        threshold_us: u64,
    ) -> Self {
        // Calculate bandwidth: MB/s = (bytes / 1_000_000) / (duration_us / 1_000_000)
        let bandwidth_mbps = if duration_us > 0 {
            (bytes as f64 * 1_000_000.0) / (duration_us as f64 * 1_048_576.0)
        } else {
            0.0
        };

        GpuMemoryTransfer {
            label,
            direction,
            bytes,
            duration_us,
            bandwidth_mbps,
            buffer_usage,
            is_slow: duration_us > threshold_us,
        }
    }
}
```

### 3.2 OTLP Exporter Extension

**File:** `src/otlp_exporter.rs` (extend existing)

```rust
impl OtlpExporter {
    /// Record a GPU memory transfer as a span (Sprint 39 - Phase 4)
    ///
    /// Exports GPU memory transfer timing (CPUâ†”GPU) captured via wall-clock measurement.
    /// Follows Sprint 37's adaptive sampling pattern.
    ///
    /// # Arguments
    ///
    /// * `transfer` - Metadata about the GPU memory transfer
    ///
    /// # Adaptive Sampling
    ///
    /// This method should only be called if duration >= threshold (default 100Î¼s).
    /// The caller (transfer tracking wrapper) handles sampling decisions.
    pub fn record_gpu_transfer(&self, transfer: GpuMemoryTransfer) {
        let mut span_attrs = vec![
            KeyValue::new("gpu_transfer.direction", transfer.direction.as_str().to_string()),
            KeyValue::new("gpu_transfer.bytes", transfer.bytes as i64),
            KeyValue::new("gpu_transfer.duration_us", transfer.duration_us as i64),
            KeyValue::new("gpu_transfer.bandwidth_mbps", transfer.bandwidth_mbps),
            KeyValue::new("gpu_transfer.is_slow", transfer.is_slow),
        ];

        // Optional buffer usage
        if let Some(ref usage) = transfer.buffer_usage {
            span_attrs.push(KeyValue::new("gpu_transfer.buffer_usage", usage.clone()));
        }

        let mut span = self
            .tracer
            .span_builder(format!("gpu_transfer: {}", transfer.label))
            .with_kind(SpanKind::Internal)
            .with_attributes(span_attrs)
            .start(&self.tracer);

        span.set_status(Status::Ok);
        span.end();
    }
}
```

### 3.3 Transfer Tracking Wrapper

**File:** `src/gpu_tracer.rs` (extend existing Phase 1 code)

```rust
#[cfg(feature = "gpu-tracing")]
impl GpuProfilerWrapper {
    /// Trace a buffer write operation (CPU â†’ GPU)
    ///
    /// # Arguments
    ///
    /// * `queue` - wgpu Queue
    /// * `buffer` - Target buffer
    /// * `offset` - Byte offset
    /// * `data` - Data to write
    /// * `label` - Transfer label for tracing
    ///
    /// # Example
    ///
    /// ```ignore
    /// gpu_tracer.traced_write_buffer(
    ///     &queue,
    ///     &vertex_buffer,
    ///     0,
    ///     &vertex_data,
    ///     "mesh_upload",
    /// );
    /// ```
    pub fn traced_write_buffer(
        &self,
        queue: &wgpu::Queue,
        buffer: &wgpu::Buffer,
        offset: wgpu::BufferAddress,
        data: &[u8],
        label: &str,
    ) {
        // Wall-clock timing (simple, accurate enough for transfers)
        let start = std::time::Instant::now();

        // Perform actual write
        queue.write_buffer(buffer, offset, data);

        let duration_us = start.elapsed().as_micros() as u64;
        let bytes = data.len();

        // Adaptive sampling: Only export if duration > threshold OR trace_all
        if self.config.trace_all || duration_us >= self.config.threshold_us {
            if let Some(ref exporter) = self.otlp_exporter {
                let transfer = GpuMemoryTransfer::new(
                    label.to_string(),
                    TransferDirection::CpuToGpu,
                    bytes,
                    duration_us,
                    None, // TODO: Extract buffer usage from buffer descriptor
                    self.config.threshold_us,
                );

                exporter.record_gpu_transfer(transfer);
            }
        }
    }

    /// Trace a buffer map operation (GPU â†’ CPU)
    ///
    /// Returns the mapped buffer slice wrapped with automatic unmap.
    ///
    /// # Example
    ///
    /// ```ignore
    /// let data = gpu_tracer.traced_map_buffer(
    ///     &buffer,
    ///     "framebuffer_readback",
    /// ).await;
    /// // Use data...
    /// // Auto-unmaps when dropped
    /// ```
    pub async fn traced_map_buffer(
        &self,
        buffer: &wgpu::Buffer,
        label: &str,
    ) -> wgpu::BufferView {
        let start = std::time::Instant::now();

        // Start async map operation
        let buffer_slice = buffer.slice(..);
        let (tx, rx) = futures::channel::oneshot::channel();

        buffer_slice.map_async(wgpu::MapMode::Read, move |result| {
            tx.send(result).unwrap();
        });

        // Wait for map to complete (this measures actual transfer time)
        rx.await.unwrap().unwrap();

        let duration_us = start.elapsed().as_micros() as u64;
        let bytes = buffer.size() as usize;

        // Record transfer
        if self.config.trace_all || duration_us >= self.config.threshold_us {
            if let Some(ref exporter) = self.otlp_exporter {
                let transfer = GpuMemoryTransfer::new(
                    label.to_string(),
                    TransferDirection::GpuToCpu,
                    bytes,
                    duration_us,
                    None,
                    self.config.threshold_us,
                );

                exporter.record_gpu_transfer(transfer);
            }
        }

        buffer_slice.get_mapped_range()
    }
}
```

### 3.4 User Integration Example

**Before (Phase 1 - kernels only):**
```rust
// Upload mesh data (no tracing)
queue.write_buffer(&vertex_buffer, 0, &vertex_data);

// Execute kernel (traced âœ…)
let mut scope = gpu_tracer.profiler_mut().scope("vertex_shader", &mut encoder);
```

**After (Phase 4 - kernels + transfers):**
```rust
// Upload mesh data (now traced âœ…)
gpu_tracer.traced_write_buffer(
    &queue,
    &vertex_buffer,
    0,
    &vertex_data,
    "mesh_data_upload",
);

// Execute kernel (traced âœ…)
let mut scope = gpu_tracer.profiler_mut().scope("vertex_shader", &mut encoder);

// Readback results (now traced âœ…)
let result_data = gpu_tracer.traced_map_buffer(
    &output_buffer,
    "result_readback",
).await;
```

---

## 4. Implementation Plan

### 4.1 Sprint 39 Checklist (Phase 4: Memory Transfers)

**RED Phase (Tests First):**

**File:** `tests/sprint39_gpu_transfer_tracking_tests.rs`

```rust
#[test]
#[cfg(all(feature = "gpu-tracing", feature = "otlp"))]
fn test_cpu_to_gpu_transfer_traced() {
    // Test that write_buffer is traced with correct attributes
}

#[test]
#[cfg(all(feature = "gpu-tracing", feature = "otlp"))]
fn test_gpu_to_cpu_transfer_traced() {
    // Test that map_async is traced
}

#[test]
#[cfg(all(feature = "gpu-tracing", feature = "otlp"))]
fn test_transfer_bandwidth_calculated() {
    // Test that bandwidth is calculated correctly
}

#[test]
#[cfg(all(feature = "gpu-tracing", feature = "otlp"))]
fn test_kernels_and_transfers_unified_trace() {
    // Test that transfers and kernels appear in same trace
}
```

**GREEN Phase (Implementation):**
1. Add `TransferDirection` enum to `src/otlp_exporter.rs` (~20 lines)
2. Add `GpuMemoryTransfer` struct to `src/otlp_exporter.rs` (~60 lines)
3. Add `record_gpu_transfer()` method to `OtlpExporter` (~40 lines)
4. Add `traced_write_buffer()` to `GpuProfilerWrapper` (~50 lines)
5. Add `traced_map_buffer()` to `GpuProfilerWrapper` (~50 lines)
6. Implement 6+ integration tests (~300 lines)

**Total Code:** ~520 lines

### 4.2 No New Dependencies

**Reuse Phase 1:**
- âœ… wgpu 23.0 (already added in Sprint 37)
- âœ… wgpu-profiler 0.18 (already added in Sprint 37)
- âœ… gpu-tracing feature flag (already defined)

**No new dependencies required!**

---

## 5. Testing Strategy

### 5.1 Integration Tests

**File:** `tests/sprint39_gpu_transfer_tracking_tests.rs`

```rust
#[test]
#[cfg(all(feature = "gpu-tracing", feature = "otlp"))]
fn test_large_transfer_traced() {
    // Upload 10MB buffer, verify:
    // - Span exists with name "gpu_transfer: large_data"
    // - direction = "cpu_to_gpu"
    // - bytes = 10485760
    // - bandwidth_mbps > 0
}

#[test]
#[cfg(all(feature = "gpu-tracing", feature = "otlp"))]
fn test_small_transfer_not_traced() {
    // Upload 100 bytes (fast transfer), verify:
    // - No span exported (adaptive sampling)
}
```

---

## 6. Performance Impact

### 6.1 Overhead Analysis

**Wall-clock timing overhead:** <1Î¼s per transfer (negligible)

**Adaptive sampling:** Same as Phase 1 (100Î¼s threshold)

**Expected overhead:** <0.5% (wall-clock timing is very cheap)

---

## Document Control

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2025-11-21 | Claude Code | Initial specification (Issue #16 Phase 4) |

**Status:** âœ… Ready for Implementation
**Dependencies:** Sprint 37 (Phase 1: wgpu kernel tracing)
**Next Steps:** Implement `GpuMemoryTransfer` struct and transfer tracking methods
